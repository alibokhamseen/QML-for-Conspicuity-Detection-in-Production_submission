{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5a2bb4",
   "metadata": {},
   "source": [
    "# Variational Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5195c3",
   "metadata": {},
   "source": [
    "In this notebook, we present an introduction to the work flow of quantum classifiers using **Pennylane**. This is supposed to be a simple guide, starting with the basics without requiring knowledge of classical machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841baa4b",
   "metadata": {},
   "source": [
    "## The Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd823618",
   "metadata": {},
   "source": [
    "Classifiers are used to create predictive models of classification problems. Our goal is to create **Variational Classifiers** that answers classification questions. \n",
    "\n",
    "Here, we build two models to answer two problems:\n",
    "- Check the parity\n",
    "- Breast Cancer Wisconsin https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33d410",
   "metadata": {},
   "source": [
    "# Parity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f5c06",
   "metadata": {},
   "source": [
    "For this problem, we are given $x$ and we have to determine $f(x)$, where $x$ is a bitstring $x \\in \\{0,1\\}^{\\otimes n}$ and $f(x)$ is the parity of $x$.\n",
    "$$f(x) =\n",
    "\\begin{cases} \n",
    "0 & \\text{if there is an even number of 1's in } x \\\\\n",
    "1 & \\text{else}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec888f",
   "metadata": {},
   "source": [
    "Our goal is to create a quantum model and teach it how to solve $f(x)$ given any $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d59f1",
   "metadata": {},
   "source": [
    "You might be wondering what the quantum model looks like. Well, the model is a quantum circuit with adjustable parameters, *weights*. We train the model by testing different combinations of these weights or you can also call them variational parameters. The quantum circuit is called *ansatz*. The german word means an *educated guess*. Different problems might benefit from using a specific type of ansatz.\n",
    "\n",
    "Choosing or developing the right ansatz isn't always simple as you need a quantum circuit with weights sensitive to your problem input, which may not always be straighforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0abbc",
   "metadata": {},
   "source": [
    "For this notebook, we need the following imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b095357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa67b7",
   "metadata": {},
   "source": [
    "and we will run our quantum circuits in the following Pennylane simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11ac81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fae22c",
   "metadata": {},
   "source": [
    "### Ansatz\n",
    "\n",
    "Our ansatz, as what you might expect, depends on the input length, `len(x)`. We will restrict ourselves to bitstrings of `len(x) = 4`.\n",
    "\n",
    "*Note: Pennylane refers to qubits as wires*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b936119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ansatz\n",
    "def layer(layer_weights):\n",
    "    for wire in range(4):\n",
    "        qml.Rot(*layer_weights[wire], wires=wire)\n",
    "\n",
    "    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n",
    "        qml.CNOT(wires)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9fbdba",
   "metadata": {},
   "source": [
    "I hope you noticed that we will use three weights in each layer, which are rotational angles. These three paramters are what we want to optimize in our ansatz. `layer` is the building block of our variational circuit. We can stack multiple layers as we optimize the weights or use a single layer. Our goal is to find optimal set of angles for a specific number of layers to evaluate the parity function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21014479",
   "metadata": {},
   "source": [
    "### How to use the input $x$?\n",
    "\n",
    "We use the following simple mapping from bitstring of length 4 to 4 qubits\n",
    "$$x = 0101 \\rightarrow |\\psi\\rangle = |0101\\rangle.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9222a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_preparation(x: list):\n",
    "    qml.BasisState(x, wires=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729af15",
   "metadata": {},
   "source": [
    "`BasisState` takes a list of integers $-$ zeros and ones $-$ to initialize qubits in the desired state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf93558d",
   "metadata": {},
   "source": [
    "Right now, we have the functions we need to create our model $-$ the quantum circuit we want to optimize its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d053d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights, x):\n",
    "    state_preparation(x)\n",
    "\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afef6e",
   "metadata": {},
   "source": [
    "Our circuit starts by preparing our qubits in the desired state. Then, add variational layers of our ansatz. In the end, we find the expectation of our 0th qubit in the Z basis, which has the range $[-1,1]$. Expectation values tells us the average of measurement results. The decorator `@qml.qnode(dev)` is used to tell `circuit` that it is preparing a quantum circuit (QNode to be precise in Pennylane's language) to run on the device we specified earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79820a",
   "metadata": {},
   "source": [
    "We call our `circuit` using `variational_classifier` that prepares the quantum circuit with some paramaters `weights`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b0f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9fcde",
   "metadata": {},
   "source": [
    "Due to the nature of our circuit, our model produces results that passes through the origin. So, we classically add an offset value `bias`, which we can think of as an additional weight we want to optimize, but classically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c384ec9",
   "metadata": {},
   "source": [
    "How can we assess the results of our model? We use a `cost` function that compares our model's results to the actual results. This means that we are doing supervised learning $-$ we know the correct answers of the training dataset and we want the model to find the same results.\n",
    "\n",
    "*Note: our goal is to minimize `cost`. `cost` tells us how far away our model is from getting the right answers.*\n",
    "\n",
    "Our model produces `predictions`. There are multiple ways to find out how bad they are. One way is to sum up the absolute value of how far away each prediction is from the right answer, or even better, square these values. One benefit of squaring these values is to increase the penalty of producing really bad answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e863a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    # We use a call to qml.math.stack to allow subtracting the arrays directly\n",
    "    # Pennylane doesn't allow us to do outside operations when evaluating the cost\n",
    "    #       Cost functions need to be efficient and as direct as possible\n",
    "    return np.mean((labels - qml.math.stack(predictions)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7daf1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b400d70",
   "metadata": {},
   "source": [
    "Our `cost` function returns a single number, where the smaller is the better. Our goal is to create a model with minimum `cost` value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902457d",
   "metadata": {},
   "source": [
    "`accuracy` just tells us the ratio of how many right answers our model got with a set of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f840fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771624e",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "\n",
    "`X` includes the input, $x$, while `Y` includes the labels, $f(x)$.\n",
    "\n",
    "$f(x)$ are given as $\\{0, 1\\}$, but we map them to $\\{-1, 1\\}$.\n",
    "\n",
    "This mapping is important because we expect deterministic results of the expectation value. If the expectation value happened to be zero, then real life measurements would be random. How do we interpret the output? If the expectation value is not zero, this means there is a favorable outcome. We run the circuit multiple times to see which outcome is more likely, and that's our answer.\n",
    "\n",
    "*Note: a quantum measurement gives us the eigenvalue of the qubit state.*\n",
    "\n",
    "*$\\{-1, 1\\}$ are the eigenvalues of qubit measurement operators*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5900920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 0 0 1], y = 1\n",
      "x = [0 0 1 0], y = 1\n",
      "x = [0 1 0 0], y = 1\n",
      "x = [0 1 0 1], y = -1\n",
      "x = [0 1 1 0], y = -1\n",
      "x = [0 1 1 1], y = 1\n",
      "x = [1 0 0 0], y = 1\n",
      "x = [1 0 0 1], y = -1\n",
      "x = [1 0 1 1], y = 1\n",
      "x = [1 1 1 1], y = -1\n"
     ]
    }
   ],
   "source": [
    "training_data = np.loadtxt(\"v1/train.txt\", dtype=int)\n",
    "X = np.array(training_data[:, :-1])\n",
    "Y = np.array(training_data[:, -1])\n",
    "Y = 2 * Y - 1 # shift label from {0, 1} to {-1, 1}\n",
    "for x,y in zip(X, Y):\n",
    "    print(f\"x = {x}, y = {y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa34ea",
   "metadata": {},
   "source": [
    "### Initial parameter preparation\n",
    "In this step, we prepare a random starting point for our ansatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b24c0fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Values of weights:\n",
      "[[[ 0.01764052  0.00400157  0.00978738]\n",
      "  [ 0.02240893  0.01867558 -0.00977278]\n",
      "  [ 0.00950088 -0.00151357 -0.00103219]\n",
      "  [ 0.00410599  0.00144044  0.01454274]]\n",
      "\n",
      " [[ 0.00761038  0.00121675  0.00443863]\n",
      "  [ 0.00333674  0.01494079 -0.00205158]\n",
      "  [ 0.00313068 -0.00854096 -0.0255299 ]\n",
      "  [ 0.00653619  0.00864436 -0.00742165]]]\n",
      "Bias:  0.5\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "\n",
    "bias_init = np.array(0.5, requires_grad=True)\n",
    "\n",
    "print(f\"Initial Values of weights:\\n{weights_init}\")\n",
    "\n",
    "print(\"Bias: \", bias_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a4677",
   "metadata": {},
   "source": [
    "We use `requires_grad=True` because that is how we minimize the `cost` function. We find a gradient of `cost` with respect to our parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bced51",
   "metadata": {},
   "source": [
    "We use `NesterovMomentumOptimizer`, an optimizer that takes into account momentum when finding the gradiant, which is a variant of the popular standard gradient descent optimizer. The `0.5` is the learning parameter speed. Steps of the generated new `weights` are proportional to the learning parameter. `batch_size` specifies how many training samples to use per iteration, from which we expand our gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e625a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ed582",
   "metadata": {},
   "source": [
    "Now, we are ready to run our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c89d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:    1 | Cost: 2.1172831 | Accuracy: 0.5000000\n",
      "Iter:    2 | Cost: 2.3066813 | Accuracy: 0.5000000\n",
      "Iter:    3 | Cost: 1.8111517 | Accuracy: 0.5000000\n",
      "Iter:    4 | Cost: 1.9491181 | Accuracy: 0.5000000\n",
      "Iter:    5 | Cost: 1.2568332 | Accuracy: 0.6000000\n",
      "Iter:    6 | Cost: 1.1503186 | Accuracy: 0.8000000\n",
      "Iter:    7 | Cost: 1.1016388 | Accuracy: 0.6000000\n",
      "Iter:    8 | Cost: 0.5680689 | Accuracy: 0.8000000\n",
      "Iter:    9 | Cost: 0.9816041 | Accuracy: 0.6000000\n",
      "Iter:   10 | Cost: 1.2509711 | Accuracy: 0.6000000\n",
      "Iter:   11 | Cost: 1.0835664 | Accuracy: 0.8000000\n",
      "Iter:   12 | Cost: 1.0306310 | Accuracy: 0.6000000\n",
      "Iter:   13 | Cost: 1.1885577 | Accuracy: 0.5000000\n",
      "Iter:   14 | Cost: 1.1708716 | Accuracy: 0.5000000\n",
      "Iter:   15 | Cost: 0.9885189 | Accuracy: 0.6000000\n",
      "Iter:   16 | Cost: 1.0195433 | Accuracy: 0.6000000\n",
      "Iter:   17 | Cost: 1.8276233 | Accuracy: 0.4000000\n",
      "Iter:   18 | Cost: 0.9853444 | Accuracy: 0.6000000\n",
      "Iter:   19 | Cost: 0.9926827 | Accuracy: 0.6000000\n",
      "Iter:   20 | Cost: 1.1642655 | Accuracy: 0.4000000\n",
      "Iter:   21 | Cost: 1.5851212 | Accuracy: 0.6000000\n",
      "Iter:   22 | Cost: 1.1610811 | Accuracy: 0.5000000\n",
      "Iter:   23 | Cost: 1.0155633 | Accuracy: 0.5000000\n",
      "Iter:   24 | Cost: 1.0157561 | Accuracy: 0.5000000\n",
      "Iter:   25 | Cost: 0.8540421 | Accuracy: 0.4000000\n",
      "Iter:   26 | Cost: 0.3915972 | Accuracy: 1.0000000\n",
      "Iter:   27 | Cost: 0.1727009 | Accuracy: 1.0000000\n",
      "Iter:   28 | Cost: 0.0987598 | Accuracy: 1.0000000\n",
      "Iter:   29 | Cost: 0.0730870 | Accuracy: 1.0000000\n",
      "Iter:   30 | Cost: 0.0704064 | Accuracy: 1.0000000\n",
      "Iter:   31 | Cost: 0.0494109 | Accuracy: 1.0000000\n",
      "Iter:   32 | Cost: 0.0250971 | Accuracy: 1.0000000\n",
      "Iter:   33 | Cost: 0.0105484 | Accuracy: 1.0000000\n",
      "Iter:   34 | Cost: 0.0050382 | Accuracy: 1.0000000\n",
      "Iter:   35 | Cost: 0.0043186 | Accuracy: 1.0000000\n",
      "Iter:   36 | Cost: 0.0060230 | Accuracy: 1.0000000\n",
      "Iter:   37 | Cost: 0.0075375 | Accuracy: 1.0000000\n",
      "Iter:   38 | Cost: 0.0067643 | Accuracy: 1.0000000\n",
      "Iter:   39 | Cost: 0.0052339 | Accuracy: 1.0000000\n",
      "Iter:   40 | Cost: 0.0043971 | Accuracy: 1.0000000\n",
      "Iter:   41 | Cost: 0.0015321 | Accuracy: 1.0000000\n",
      "Iter:   42 | Cost: 0.0008679 | Accuracy: 1.0000000\n",
      "Iter:   43 | Cost: 0.0005969 | Accuracy: 1.0000000\n",
      "Iter:   44 | Cost: 0.0005272 | Accuracy: 1.0000000\n",
      "Iter:   45 | Cost: 0.0003713 | Accuracy: 1.0000000\n",
      "Iter:   46 | Cost: 0.0003988 | Accuracy: 1.0000000\n",
      "Iter:   47 | Cost: 0.0005541 | Accuracy: 1.0000000\n",
      "Iter:   48 | Cost: 0.0007634 | Accuracy: 1.0000000\n",
      "Iter:   49 | Cost: 0.0006621 | Accuracy: 1.0000000\n",
      "Iter:   50 | Cost: 0.0008535 | Accuracy: 1.0000000\n",
      "Iter:   51 | Cost: 0.0007322 | Accuracy: 1.0000000\n",
      "Iter:   52 | Cost: 0.0007388 | Accuracy: 1.0000000\n",
      "Iter:   53 | Cost: 0.0005273 | Accuracy: 1.0000000\n",
      "Iter:   54 | Cost: 0.0004683 | Accuracy: 1.0000000\n",
      "Iter:   55 | Cost: 0.0003548 | Accuracy: 1.0000000\n",
      "Iter:   56 | Cost: 0.0002629 | Accuracy: 1.0000000\n",
      "Iter:   57 | Cost: 0.0001790 | Accuracy: 1.0000000\n",
      "Iter:   58 | Cost: 0.0001470 | Accuracy: 1.0000000\n",
      "Iter:   59 | Cost: 0.0000971 | Accuracy: 1.0000000\n",
      "Iter:   60 | Cost: 0.0000735 | Accuracy: 1.0000000\n",
      "Iter:   61 | Cost: 0.0000532 | Accuracy: 1.0000000\n",
      "Iter:   62 | Cost: 0.0000430 | Accuracy: 1.0000000\n",
      "Iter:   63 | Cost: 0.0000347 | Accuracy: 1.0000000\n",
      "Iter:   64 | Cost: 0.0000328 | Accuracy: 1.0000000\n",
      "Iter:   65 | Cost: 0.0000281 | Accuracy: 1.0000000\n",
      "Iter:   66 | Cost: 0.0000225 | Accuracy: 1.0000000\n",
      "Iter:   67 | Cost: 0.0000233 | Accuracy: 1.0000000\n",
      "Iter:   68 | Cost: 0.0000196 | Accuracy: 1.0000000\n",
      "Iter:   69 | Cost: 0.0000180 | Accuracy: 1.0000000\n",
      "Iter:   70 | Cost: 0.0000190 | Accuracy: 1.0000000\n",
      "Iter:   71 | Cost: 0.0000169 | Accuracy: 1.0000000\n",
      "Iter:   72 | Cost: 0.0000166 | Accuracy: 1.0000000\n",
      "Iter:   73 | Cost: 0.0000146 | Accuracy: 1.0000000\n",
      "Iter:   74 | Cost: 0.0000175 | Accuracy: 1.0000000\n",
      "Iter:   75 | Cost: 0.0000134 | Accuracy: 1.0000000\n",
      "Iter:   76 | Cost: 0.0000133 | Accuracy: 1.0000000\n",
      "Iter:   77 | Cost: 0.0000130 | Accuracy: 1.0000000\n",
      "Iter:   78 | Cost: 0.0000117 | Accuracy: 1.0000000\n",
      "Iter:   79 | Cost: 0.0000104 | Accuracy: 1.0000000\n",
      "Iter:   80 | Cost: 0.0000111 | Accuracy: 1.0000000\n",
      "Iter:   81 | Cost: 0.0000089 | Accuracy: 1.0000000\n",
      "Iter:   82 | Cost: 0.0000085 | Accuracy: 1.0000000\n",
      "Iter:   83 | Cost: 0.0000078 | Accuracy: 1.0000000\n",
      "Iter:   84 | Cost: 0.0000070 | Accuracy: 1.0000000\n",
      "Iter:   85 | Cost: 0.0000064 | Accuracy: 1.0000000\n",
      "Iter:   86 | Cost: 0.0000105 | Accuracy: 1.0000000\n",
      "Iter:   87 | Cost: 0.0000062 | Accuracy: 1.0000000\n",
      "Iter:   88 | Cost: 0.0000053 | Accuracy: 1.0000000\n",
      "Iter:   89 | Cost: 0.0000052 | Accuracy: 1.0000000\n",
      "Iter:   90 | Cost: 0.0000041 | Accuracy: 1.0000000\n",
      "Iter:   91 | Cost: 0.0000052 | Accuracy: 1.0000000\n",
      "Iter:   92 | Cost: 0.0000035 | Accuracy: 1.0000000\n",
      "Iter:   93 | Cost: 0.0000033 | Accuracy: 1.0000000\n",
      "Iter:   94 | Cost: 0.0000030 | Accuracy: 1.0000000\n",
      "Iter:   95 | Cost: 0.0000028 | Accuracy: 1.0000000\n",
      "Iter:   96 | Cost: 0.0000025 | Accuracy: 1.0000000\n",
      "Iter:   97 | Cost: 0.0000036 | Accuracy: 1.0000000\n",
      "Iter:   98 | Cost: 0.0000024 | Accuracy: 1.0000000\n",
      "Iter:   99 | Cost: 0.0000023 | Accuracy: 1.0000000\n",
      "Iter:  100 | Cost: 0.0000019 | Accuracy: 1.0000000\n",
      "Iter:  101 | Cost: 0.0000017 | Accuracy: 1.0000000\n",
      "Iter:  102 | Cost: 0.0000016 | Accuracy: 1.0000000\n",
      "Iter:  103 | Cost: 0.0000014 | Accuracy: 1.0000000\n",
      "Iter:  104 | Cost: 0.0000015 | Accuracy: 1.0000000\n",
      "Iter:  105 | Cost: 0.0000013 | Accuracy: 1.0000000\n",
      "Iter:  106 | Cost: 0.0000011 | Accuracy: 1.0000000\n",
      "Iter:  107 | Cost: 0.0000012 | Accuracy: 1.0000000\n",
      "Iter:  108 | Cost: 0.0000010 | Accuracy: 1.0000000\n",
      "Iter:  109 | Cost: 0.0000010 | Accuracy: 1.0000000\n",
      "Iter:  110 | Cost: 0.0000009 | Accuracy: 1.0000000\n",
      "Iter:  111 | Cost: 0.0000008 | Accuracy: 1.0000000\n",
      "Iter:  112 | Cost: 0.0000008 | Accuracy: 1.0000000\n",
      "Iter:  113 | Cost: 0.0000007 | Accuracy: 1.0000000\n",
      "Iter:  114 | Cost: 0.0000007 | Accuracy: 1.0000000\n",
      "Iter:  115 | Cost: 0.0000007 | Accuracy: 1.0000000\n",
      "Iter:  116 | Cost: 0.0000007 | Accuracy: 1.0000000\n",
      "Iter:  117 | Cost: 0.0000006 | Accuracy: 1.0000000\n",
      "Iter:  118 | Cost: 0.0000006 | Accuracy: 1.0000000\n",
      "Iter:  119 | Cost: 0.0000006 | Accuracy: 1.0000000\n",
      "Iter:  120 | Cost: 0.0000005 | Accuracy: 1.0000000\n",
      "Iter:  121 | Cost: 0.0000005 | Accuracy: 1.0000000\n",
      "Iter:  122 | Cost: 0.0000006 | Accuracy: 1.0000000\n",
      "Iter:  123 | Cost: 0.0000005 | Accuracy: 1.0000000\n",
      "Iter:  124 | Cost: 0.0000005 | Accuracy: 1.0000000\n",
      "Iter:  125 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  126 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  127 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  128 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  129 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  130 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  131 | Cost: 0.0000005 | Accuracy: 1.0000000\n",
      "Iter:  132 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  133 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  134 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  135 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  136 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  137 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  138 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  139 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  140 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  141 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  142 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  143 | Cost: 0.0000002 | Accuracy: 1.0000000\n",
      "Iter:  144 | Cost: 0.0000002 | Accuracy: 1.0000000\n",
      "Iter:  145 | Cost: 0.0000002 | Accuracy: 1.0000000\n",
      "Iter:  146 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  147 | Cost: 0.0000002 | Accuracy: 1.0000000\n",
      "Iter:  148 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  149 | Cost: 0.0000002 | Accuracy: 1.0000000\n",
      "Iter:  150 | Cost: 0.0000002 | Accuracy: 1.0000000\n"
     ]
    }
   ],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(150):\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "\n",
    "    current_cost = cost(weights, bias, X, Y)\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335f4f5",
   "metadata": {},
   "source": [
    "Note that we reached perfect accuracy on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69064c0",
   "metadata": {},
   "source": [
    "### Test with new data\n",
    "It is important to experiment on data our model hasn't seen befor. Sometimes, we might achieve high accuracy on the training data but create a useless model for new input. This is called overtraining, where we force our model to adapt to the training set not the actual problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e6b6e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 0 0 0], y = -1, pred=-1.0\n",
      "x = [0 0 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 1 0], y = 1, pred=1.0\n",
      "x = [1 1 0 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 1], y = 1, pred=1.0\n",
      "Accuracy on unseen data: 1.0\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"v1/test.txt\", dtype=int)\n",
    "X_test = np.array(data[:, :-1])\n",
    "Y_test = np.array(data[:, -1])\n",
    "Y_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "\n",
    "for x,y,p in zip(X_test, Y_test, predictions_test):\n",
    "    print(f\"x = {x}, y = {y}, pred={p}\")\n",
    "\n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"Accuracy on unseen data:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e5845",
   "metadata": {},
   "source": [
    "After creating this simple model, next, we create a more sophisticated model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7684d3",
   "metadata": {},
   "source": [
    "# Breast Cancer Wisconsin\n",
    "Breast Cancer Wisconsin original dataset: https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d62d2c",
   "metadata": {},
   "source": [
    "To classify this dataset, we have to deal with a more complicated input. We will use a similar ansatz, but we have to find a way to initialize quantum states. So far, from what we know, our ansatz should be sensitive to the input to find patterns. Also, our state preparation routine should be expressive in the eyes of our ansatz. \n",
    "\n",
    "We will first define how to initialize our data. Then, we show our ansatz, after which, we produce our model and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a5763",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "*Preparing initial quantum state*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4ac4e",
   "metadata": {},
   "source": [
    "For each entry, we have 9 inputs, which are called *features* in machine learning context. Refer to the source to inspect the data. They are all integers $ \\in \\{1, 2, ..., 10\\}$\n",
    "https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7291eb3",
   "metadata": {},
   "source": [
    "To encode 9 features in a quantum circuit, I follow a routine called *amplitude encoding*. There are other routines like *angle encoding*, which might be beneficial depending on the situation, but I will stick with amplitude encoding for simplicity.\n",
    "\n",
    "Amplitude encoding routine maps each feature to a state in our system's Hilbert space. Since we have 9 features, this means we need at least 4 qubits to create a quantum system with a Hilbert space of dimension-16. So, we end up with 7 more states. Can we use the additional states to encode more information? Of course, we can extrapolate new features from the data we have. Note that we have to normalize our features befor encoding them. Normalization can erase information about the actual length of the input. To circumvent this issue, we pad each set of features with their magnitude. This way, the added feature stores some information about the actual magnitude of the input, which may be important in some cases.\n",
    "\n",
    "Steps:\n",
    "- Find the magnitudes of each features set\n",
    "- Map each feature from integers $ \\in \\{1, 2, ..., 10\\}$ to the range $[-1,1]$. We use negative values because amplitudes can be negative or even complex, but we don't make use of complex encoding in this tutorial. \n",
    "- Pad the magnitude and 6 zeros to create vectors with $dimension = 16 $\n",
    "- Normalize \n",
    "\n",
    "As to our labels - the solutions - they are given as $\\{2,4\\}$, but again, we map them to $\\{-1,1\\}$ because qubit measurements yield unitary eignvalues, which are $\\{-1,1\\}$.\n",
    "\n",
    "*Note: we replaced missing values with 5.5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4488bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First entry features: [-0.13343587 -0.13343587 -0.10378346 -0.13343587 -0.07413104  0.\n",
      " -0.13343587 -0.13343587 -0.13343587  0.93643088  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "First entry label: -1.0\n"
     ]
    }
   ],
   "source": [
    "# There are some missing values we replaced them with 5.5\n",
    "table =np.genfromtxt(\"v2/breast+cancer+wisconsin+original/breast-cancer-wisconsin.data\", \n",
    "                     delimiter=',', missing_values='?', filling_values=5.5, dtype=float)\n",
    "\n",
    "labelsL = table[:,-1] - 3 # Shift {2,4} -> {-1,1}\n",
    "featuresL = table[:,1:-1]\n",
    "# These are the magnitudes of each vector in featuresL\n",
    "magL = np.linalg.norm(featuresL, axis=1)\n",
    "# Map features [1, 10] -> [-1,1]\n",
    "min_vals = np.min(featuresL, axis=0)\n",
    "max_vals = np.max(featuresL, axis=0)\n",
    "featuresL = 2 * (featuresL - min_vals) / (max_vals - min_vals) - 1\n",
    "\n",
    "# We pad features in featuresL with the corresponding value from magL then add zeros\n",
    "padding_0s = np.zeros((len(featuresL), 6)) \n",
    "padding_mag = np.array([np.full(1, mag) for mag in magL])\n",
    "padding = np.c_[padding_mag, padding_0s]\n",
    "padd_featuresL = np.c_[featuresL, padding]\n",
    "\n",
    "# After padding, we normalize\n",
    "norm = np.sqrt(np.sum(padd_featuresL**2, -1))\n",
    "norm_padd_featuresL = (padd_featuresL.T / norm).T\n",
    "\n",
    "# Split to training and test sets\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(featuresL))\n",
    "split_index = int(len(norm_padd_featuresL) * 0.8) # 80% for training\n",
    "train_indices, test_indices = indices[:split_index], indices[split_index:]\n",
    "featuresL_train = norm_padd_featuresL[train_indices]\n",
    "featuresL_test = norm_padd_featuresL[test_indices]\n",
    "labelsL_train, labelsL_test = labelsL[train_indices], labelsL[test_indices]\n",
    "\n",
    "\n",
    "print(\"First entry features:\", featuresL_train[0])\n",
    "print(\"First entry label:\", labelsL_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fcb35c",
   "metadata": {},
   "source": [
    "### Preparing Our Circuit\n",
    "`AmplitudeEmbedding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ea68f",
   "metadata": {},
   "source": [
    "We showed that we are going to use 4 qubits to encode a state of $dimension = 16$ . Preparing such states on quantum devices isn't a simple taks, but Pennylane provides a shortcut we can use to prepare an arbitrary state `AmplitudeEmbedding`. When using real quantum devices, we need to generate our arbitrary state, which Pennylane still can do for us. As for our variational layer, we use the same layer we prepared earlier to manipulate 4 qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1751a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "# Define the state preparation function\n",
    "def state_preparation(features):\n",
    "    num_qubits = 4\n",
    "    qml.AmplitudeEmbedding(features, wires=[q for q in range(num_qubits)])\n",
    "\n",
    "# Define the variational layer\n",
    "def layer(layer_weights):\n",
    "    for i in range(num_qubits):\n",
    "        qml.Rot(*layer_weights[i], wires=i)\n",
    "    for i in range(num_qubits):\n",
    "        qml.CNOT(wires=[i, (i + 1) % num_qubits])\n",
    "\n",
    "# Define the variational circuit\n",
    "@qml.qnode(dev)\n",
    "def circuit(weights, features):\n",
    "    state_preparation(features)\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights)\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c6165",
   "metadata": {},
   "source": [
    "Using the same functions from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abdf0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labelsL, predictions):\n",
    "    # We use a call to qml.math.stack to allow subtracting the arrays directly\n",
    "    return np.mean((labelsL - qml.math.stack(predictions)) ** 2)\n",
    "\n",
    "def variational_classifier(weights, bias, features):\n",
    "    prediction = np.array(circuit(weights, features))\n",
    "    return prediction + bias\n",
    "\n",
    "def cost(weights, bias, featuresL, labelsL):\n",
    "    predictions = np.array([variational_classifier(weights, bias, features) for features in featuresL], requires_grad=True)\n",
    "    return square_loss(labelsL, predictions)\n",
    "\n",
    "def accuracy(labelsL, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labelsL, predictions))\n",
    "    acc = acc / len(labelsL)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296cb91",
   "metadata": {},
   "source": [
    "### Run\n",
    "Now, that we have everything ready, we can train our classifier. Of course, there are multiple parameters to play with here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd887727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:    1 | Cost: 1.8067526 | Accuracy: 0.3416816\n",
      "Iter:    2 | Cost: 1.3001032 | Accuracy: 0.3416816\n",
      "Iter:    3 | Cost: 0.6507062 | Accuracy: 0.8425760\n",
      "Iter:    4 | Cost: 0.4554686 | Accuracy: 0.9338104\n",
      "Iter:    5 | Cost: 0.4930879 | Accuracy: 0.6583184\n",
      "Iter:    6 | Cost: 0.5434006 | Accuracy: 0.6583184\n",
      "Iter:    7 | Cost: 0.5607458 | Accuracy: 0.6583184\n",
      "Iter:    8 | Cost: 0.5131475 | Accuracy: 0.6583184\n",
      "Iter:    9 | Cost: 0.4658143 | Accuracy: 0.6976744\n",
      "Iter:   10 | Cost: 0.4412002 | Accuracy: 0.8747764\n",
      "Iter:   11 | Cost: 0.4306980 | Accuracy: 0.9534884\n",
      "Iter:   12 | Cost: 0.4410185 | Accuracy: 0.9731664\n",
      "Iter:   13 | Cost: 0.4642192 | Accuracy: 0.9713775\n",
      "Iter:   14 | Cost: 0.4993349 | Accuracy: 0.9409660\n",
      "Iter:   15 | Cost: 0.4777656 | Accuracy: 0.9427549\n",
      "Iter:   16 | Cost: 0.4420767 | Accuracy: 0.9624329\n",
      "Iter:   17 | Cost: 0.4153720 | Accuracy: 0.9713775\n",
      "Iter:   18 | Cost: 0.3838269 | Accuracy: 0.9731664\n",
      "Iter:   19 | Cost: 0.3757428 | Accuracy: 0.9642218\n",
      "Iter:   20 | Cost: 0.3876628 | Accuracy: 0.9338104\n",
      "Iter:   21 | Cost: 0.3985731 | Accuracy: 0.8765653\n",
      "Iter:   22 | Cost: 0.4124850 | Accuracy: 0.8228980\n",
      "Iter:   23 | Cost: 0.4095852 | Accuracy: 0.8372093\n",
      "Iter:   24 | Cost: 0.3862834 | Accuracy: 0.9087657\n",
      "Iter:   25 | Cost: 0.3620078 | Accuracy: 0.9624329\n",
      "Iter:   26 | Cost: 0.3576161 | Accuracy: 0.9713775\n",
      "Iter:   27 | Cost: 0.3636369 | Accuracy: 0.9749553\n",
      "Iter:   28 | Cost: 0.3672711 | Accuracy: 0.9731664\n",
      "Iter:   29 | Cost: 0.3664524 | Accuracy: 0.9713775\n",
      "Iter:   30 | Cost: 0.3558292 | Accuracy: 0.9731664\n",
      "Iter:   31 | Cost: 0.3459239 | Accuracy: 0.9731664\n",
      "Iter:   32 | Cost: 0.3606359 | Accuracy: 0.9481216\n",
      "Iter:   33 | Cost: 0.4060124 | Accuracy: 0.8425760\n",
      "Iter:   34 | Cost: 0.4633304 | Accuracy: 0.6905188\n",
      "Iter:   35 | Cost: 0.4409744 | Accuracy: 0.7209302\n",
      "Iter:   36 | Cost: 0.3955927 | Accuracy: 0.8676208\n",
      "Iter:   37 | Cost: 0.3642197 | Accuracy: 0.9445438\n",
      "Iter:   38 | Cost: 0.3444471 | Accuracy: 0.9731664\n",
      "Iter:   39 | Cost: 0.3507657 | Accuracy: 0.9749553\n",
      "Iter:   40 | Cost: 0.3490756 | Accuracy: 0.9749553\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(99)\n",
    "num_qubits = 4\n",
    "num_layers = 6\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "opt = NesterovMomentumOptimizer(0.05)\n",
    "batch_size = 12\n",
    "\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(40):\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(featuresL_train), (batch_size,))\n",
    "    X_batch = featuresL_train[batch_index]\n",
    "    Y_batch = labelsL_train[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, featuresL=X_batch, labelsL=Y_batch)\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in featuresL_train]\n",
    "    current_cost = cost(weights, bias, featuresL_train, labelsL_train)\n",
    "    acc = accuracy(labelsL_train, predictions)\n",
    "\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5aaa28",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7674df",
   "metadata": {},
   "source": [
    "We didn't reach 100% accuracy, but that is fine. Let's check the performance of our model on test data.\n",
    "\n",
    "*If you want, you can compare our model to other baseline classical models in https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2378bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9571\n"
     ]
    }
   ],
   "source": [
    "predictions = [np.sign(variational_classifier(weights, bias, x)) for x in featuresL_test]\n",
    "acc = accuracy(labelsL_test, predictions)\n",
    "print(\"Accuracy on test data:\", f\"{acc:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da48e1",
   "metadata": {},
   "source": [
    "That looks impressive, but is there anything we can do to improve our model? Let's take a closer look at `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d3a87b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.32216623e-02, -1.00384775e-01, -8.87378024e-03],\n",
       "         [ 2.01177902e-02,  5.41638977e-01,  5.81053787e-03],\n",
       "         [-6.87783737e-03,  4.00790115e-01, -9.21608697e-03],\n",
       "         [-2.58377066e-02, -1.96200056e-01,  6.46627550e-03]],\n",
       "\n",
       "        [[ 1.37897366e-03,  5.53737071e-01,  5.89187059e-03],\n",
       "         [-1.85980721e-02,  8.02055393e-02, -2.37206576e-03],\n",
       "         [ 1.89052074e-03,  4.34496436e-01,  5.11194275e-03],\n",
       "         [ 1.66626709e-02, -3.41487029e-01, -8.38294968e-04]],\n",
       "\n",
       "        [[-1.00681470e-02, -6.64007460e-01, -9.84632737e-03],\n",
       "         [-1.61546860e-02, -3.64667597e-01, -1.51402806e-03],\n",
       "         [ 3.00764824e-03, -3.18093386e-01,  2.66194161e-02],\n",
       "         [-4.17883909e-03, -9.16658713e-03, -5.03865929e-03]],\n",
       "\n",
       "        [[-1.30847381e-02, -1.59851960e-01, -1.66705056e-02],\n",
       "         [ 9.67586142e-03, -4.75333118e-01, -8.31843380e-04],\n",
       "         [ 1.07620235e-02,  3.23709591e-03, -1.94755958e-02],\n",
       "         [ 3.86263584e-03,  3.23025728e-01, -6.37953727e-03]],\n",
       "\n",
       "        [[ 4.02659322e-03, -2.04056245e-01, -6.50223830e-03],\n",
       "         [-7.08510608e-03,  5.27759337e-02,  1.73977666e-03],\n",
       "         [-1.47525352e-03, -9.63353033e-02,  8.88180754e-03],\n",
       "         [ 2.56468689e-02,  1.28366350e-01,  1.48426076e-03]],\n",
       "\n",
       "        [[-3.75642199e-03, -2.52104299e-03, -1.38961345e-02],\n",
       "         [ 9.37078352e-03,  2.29527913e-01,  1.28790496e-02],\n",
       "         [ 3.35122883e-04, -3.75035949e-01, -3.09956637e-03],\n",
       "         [-1.36772281e-03, -2.61895949e-02, -6.08983688e-03]]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04811cd8",
   "metadata": {},
   "source": [
    "As you can see, there are many small angles, which you might expect to be useless. Let's add a filter and round everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cda84e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after filtering and rounding:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0. , -0.1,  0. ],\n",
       "         [ 0. ,  0.5,  0. ],\n",
       "         [ 0. ,  0.4,  0. ],\n",
       "         [ 0. , -0.2,  0. ]],\n",
       "\n",
       "        [[ 0. ,  0.6,  0. ],\n",
       "         [ 0. ,  0. ,  0. ],\n",
       "         [ 0. ,  0.4,  0. ],\n",
       "         [ 0. , -0.3,  0. ]],\n",
       "\n",
       "        [[ 0. , -0.7,  0. ],\n",
       "         [ 0. , -0.4,  0. ],\n",
       "         [ 0. , -0.3,  0. ],\n",
       "         [ 0. ,  0. ,  0. ]],\n",
       "\n",
       "        [[ 0. , -0.2,  0. ],\n",
       "         [ 0. , -0.5,  0. ],\n",
       "         [ 0. ,  0. ,  0. ],\n",
       "         [ 0. ,  0.3,  0. ]],\n",
       "\n",
       "        [[ 0. , -0.2,  0. ],\n",
       "         [ 0. ,  0. ,  0. ],\n",
       "         [ 0. ,  0. ,  0. ],\n",
       "         [ 0. ,  0.1,  0. ]],\n",
       "\n",
       "        [[ 0. ,  0. ,  0. ],\n",
       "         [ 0. ,  0.2,  0. ],\n",
       "         [ 0. , -0.4,  0. ],\n",
       "         [ 0. ,  0. ,  0. ]]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = .1 # filter small angles\n",
    "weights_filter = weights.copy()\n",
    "for idx0, w_l in enumerate(weights_filter):\n",
    "    for idx1, l in enumerate(w_l):\n",
    "        for idx2, ang in enumerate(l):\n",
    "            if abs(ang) < pr:\n",
    "                weights_filter[idx0][idx1][idx2] = 0\n",
    "            elif abs(abs(ang) - np.pi/2) < pr:\n",
    "                weights_filter[idx0][idx1][idx2] = np.pi/2 * abs(ang)/ang\n",
    "            elif abs(abs(ang) - np.pi) < pr:\n",
    "                weights_filter[idx0][idx1][idx2] = np.pi * abs(ang)/ang\n",
    "            else: \n",
    "                weights_filter[idx0][idx1][idx2] = np.round(ang,1)\n",
    "print(\"Weights after filtering and rounding:\")\n",
    "weights_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f735978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data with filter: 0.9571 \n",
      "Accuracy without a filter        : 0.9571\n"
     ]
    }
   ],
   "source": [
    "predictions = [np.sign(variational_classifier(weights_filter, bias, x)) for x in featuresL_test]\n",
    "acc_f = accuracy(labelsL_test, predictions)\n",
    "print(\"Accuracy on test data with filter:\", f\"{acc_f:0.4f}\", \"\\nAccuracy without a filter        :\", f\"{acc:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ef180",
   "metadata": {},
   "source": [
    "Well, this surprised me. Our model only does rotations around one axis - y - out of three options. I am not sure what this means, but this observation might allow us to train some models faster if we can ignore some variational parameters. Though, keep in mind that what appears to be useless parameters might play an important rule in helping our optimizer escape local minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
