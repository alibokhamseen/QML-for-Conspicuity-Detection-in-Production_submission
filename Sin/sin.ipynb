{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find $sin(x)$ for any $x\\in [0,2\\pi]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate $sin$, we follow three approaches:\n",
    "- using the expectation value of a single qubit\n",
    "- using binary fractional approximation \n",
    "- Quantum models as Fourier series (https://pennylane.ai/qml/demos/tutorial_expressivity_fourier_series/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# We use random data points with random spacing to make sure \n",
    "#   our model understands the behavior of sin everywhere\n",
    "num_p = 100 # number of data points\n",
    "features = np.random.uniform(0, 2 * np.pi, num_p)\n",
    "labels = np.sin(features)\n",
    "\n",
    "# Shuffle and split the data into training and test sets\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(num_p)\n",
    "split_index = int(num_p * 0.8) # 80% for training\n",
    "train_indices, test_indices = indices[:split_index], indices[split_index:]\n",
    "features_train = features[train_indices]\n",
    "features_test = features[test_indices]\n",
    "labels_train = labels[train_indices]\n",
    "labels_test = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation value encoding $-$ single qubit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input:\n",
    "`state_preparation` rely on one dimensional feature $x$. So, we only need one qubit. Since $x\\in [0,2\\pi]$, we apply one rotaion in `state_preparation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 1\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state preparation function\n",
    "def state_preparation(feature):\n",
    "    qml.RY(feature, wires=0)\n",
    "    \n",
    "# Define the variational layer    \n",
    "def layer(layer_weights):\n",
    "    num_qubits = 1\n",
    "    for i in range(num_qubits):\n",
    "        qml.Rot(*layer_weights[i], wires=i)\n",
    "\n",
    "# Define the variational circuit\n",
    "@qml.qnode(dev)\n",
    "def circuit(weights, feature):\n",
    "    state_preparation(feature)\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights)\n",
    "    return qml.expval(qml.PauliZ(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The meaning of our output\n",
    "We take the expectation value as the solution, and we don't assume it to be $\\{-1,1\\}$ but in the range $[-1,1]$. In real life, this means that after creating the model, to use it, we run our circuit many times to approximate the solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our approach, we have to modify our `accuracy` function since we don't expect deterministic results. So, we set a `precision` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    return np.mean((labels - qml.math.stack(predictions)) ** 2)\n",
    "\n",
    "def variational_classifier(weights, bias, features):\n",
    "    prediction = np.array(circuit(weights, features))\n",
    "    return prediction + bias\n",
    "\n",
    "def cost(weights, bias, features, labels):\n",
    "    predictions = np.array([variational_classifier(weights, bias, feature) for feature in features], requires_grad=True)\n",
    "    return square_loss(labels, predictions)\n",
    "\n",
    "def accuracy(labels, predictions, precision=0.001):\n",
    "    acc = sum(abs(l - p) < precision for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 1\n",
    "num_layers = 4\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:    1 | Cost: 1.0655053 | Accuracy: 0.0000000\n",
      "Iter:    2 | Cost: 1.0347397 | Accuracy: 0.0000000\n",
      "Iter:    3 | Cost: 0.9990782 | Accuracy: 0.0000000\n",
      "Iter:    4 | Cost: 0.9491864 | Accuracy: 0.0000000\n",
      "Iter:    5 | Cost: 0.8835238 | Accuracy: 0.0000000\n",
      "Iter:    6 | Cost: 0.8146962 | Accuracy: 0.0000000\n",
      "Iter:    7 | Cost: 0.7453175 | Accuracy: 0.0000000\n",
      "Iter:    8 | Cost: 0.6687105 | Accuracy: 0.0000000\n",
      "Iter:    9 | Cost: 0.5850933 | Accuracy: 0.0125000\n",
      "Iter:   10 | Cost: 0.5051143 | Accuracy: 0.0000000\n",
      "Iter:   11 | Cost: 0.4241238 | Accuracy: 0.0000000\n",
      "Iter:   12 | Cost: 0.3472919 | Accuracy: 0.0000000\n",
      "Iter:   13 | Cost: 0.2752557 | Accuracy: 0.0000000\n",
      "Iter:   14 | Cost: 0.2097363 | Accuracy: 0.0000000\n",
      "Iter:   15 | Cost: 0.1537995 | Accuracy: 0.0000000\n",
      "Iter:   16 | Cost: 0.1090033 | Accuracy: 0.0000000\n",
      "Iter:   17 | Cost: 0.0714791 | Accuracy: 0.0000000\n",
      "Iter:   18 | Cost: 0.0429970 | Accuracy: 0.0000000\n",
      "Iter:   19 | Cost: 0.0222916 | Accuracy: 0.0000000\n",
      "Iter:   20 | Cost: 0.0089702 | Accuracy: 0.0125000\n",
      "Iter:   21 | Cost: 0.0020099 | Accuracy: 0.0250000\n",
      "Iter:   22 | Cost: 0.0001006 | Accuracy: 0.0000000\n",
      "Iter:   23 | Cost: 0.0019067 | Accuracy: 0.0000000\n",
      "Iter:   24 | Cost: 0.0060876 | Accuracy: 0.0000000\n",
      "Iter:   25 | Cost: 0.0113923 | Accuracy: 0.0000000\n",
      "Iter:   26 | Cost: 0.0168514 | Accuracy: 0.0000000\n",
      "Iter:   27 | Cost: 0.0215417 | Accuracy: 0.0000000\n",
      "Iter:   28 | Cost: 0.0258155 | Accuracy: 0.0000000\n",
      "Iter:   29 | Cost: 0.0285334 | Accuracy: 0.0000000\n",
      "Iter:   30 | Cost: 0.0298539 | Accuracy: 0.0000000\n",
      "Iter:   31 | Cost: 0.0298951 | Accuracy: 0.0125000\n",
      "Iter:   32 | Cost: 0.0288890 | Accuracy: 0.0000000\n",
      "Iter:   33 | Cost: 0.0273963 | Accuracy: 0.0000000\n",
      "Iter:   34 | Cost: 0.0251716 | Accuracy: 0.0000000\n",
      "Iter:   35 | Cost: 0.0227048 | Accuracy: 0.0000000\n",
      "Iter:   36 | Cost: 0.0200485 | Accuracy: 0.0125000\n",
      "Iter:   37 | Cost: 0.0169360 | Accuracy: 0.0000000\n",
      "Iter:   38 | Cost: 0.0141800 | Accuracy: 0.0000000\n",
      "Iter:   39 | Cost: 0.0116013 | Accuracy: 0.0125000\n",
      "Iter:   40 | Cost: 0.0093760 | Accuracy: 0.0125000\n",
      "Iter:   41 | Cost: 0.0074409 | Accuracy: 0.0125000\n",
      "Iter:   42 | Cost: 0.0058167 | Accuracy: 0.0375000\n",
      "Iter:   43 | Cost: 0.0045309 | Accuracy: 0.0000000\n",
      "Iter:   44 | Cost: 0.0035437 | Accuracy: 0.0000000\n",
      "Iter:   45 | Cost: 0.0027961 | Accuracy: 0.0000000\n",
      "Iter:   46 | Cost: 0.0022682 | Accuracy: 0.0000000\n",
      "Iter:   47 | Cost: 0.0019181 | Accuracy: 0.0000000\n",
      "Iter:   48 | Cost: 0.0016897 | Accuracy: 0.0000000\n",
      "Iter:   49 | Cost: 0.0015287 | Accuracy: 0.0000000\n",
      "Iter:   50 | Cost: 0.0014168 | Accuracy: 0.0000000\n",
      "Iter:   51 | Cost: 0.0013386 | Accuracy: 0.0375000\n",
      "Iter:   52 | Cost: 0.0012718 | Accuracy: 0.0625000\n",
      "Iter:   53 | Cost: 0.0012124 | Accuracy: 0.0125000\n",
      "Iter:   54 | Cost: 0.0011669 | Accuracy: 0.0125000\n",
      "Iter:   55 | Cost: 0.0010964 | Accuracy: 0.0125000\n",
      "Iter:   56 | Cost: 0.0009899 | Accuracy: 0.0250000\n",
      "Iter:   57 | Cost: 0.0008730 | Accuracy: 0.0250000\n",
      "Iter:   58 | Cost: 0.0007514 | Accuracy: 0.0250000\n",
      "Iter:   59 | Cost: 0.0006324 | Accuracy: 0.0000000\n",
      "Iter:   60 | Cost: 0.0005215 | Accuracy: 0.0000000\n",
      "Iter:   61 | Cost: 0.0004230 | Accuracy: 0.0125000\n",
      "Iter:   62 | Cost: 0.0003371 | Accuracy: 0.0125000\n",
      "Iter:   63 | Cost: 0.0002617 | Accuracy: 0.0250000\n",
      "Iter:   64 | Cost: 0.0001992 | Accuracy: 0.0250000\n",
      "Iter:   65 | Cost: 0.0001525 | Accuracy: 0.0500000\n",
      "Iter:   66 | Cost: 0.0001171 | Accuracy: 0.0750000\n",
      "Iter:   67 | Cost: 0.0000943 | Accuracy: 0.1125000\n",
      "Iter:   68 | Cost: 0.0000808 | Accuracy: 0.1125000\n",
      "Iter:   69 | Cost: 0.0000748 | Accuracy: 0.0000000\n",
      "Iter:   70 | Cost: 0.0000741 | Accuracy: 0.0000000\n",
      "Iter:   71 | Cost: 0.0000781 | Accuracy: 0.0000000\n",
      "Iter:   72 | Cost: 0.0000851 | Accuracy: 0.0000000\n",
      "Iter:   73 | Cost: 0.0000946 | Accuracy: 0.0000000\n",
      "Iter:   74 | Cost: 0.0001035 | Accuracy: 0.0000000\n",
      "Iter:   75 | Cost: 0.0001113 | Accuracy: 0.0000000\n",
      "Iter:   76 | Cost: 0.0001169 | Accuracy: 0.1375000\n",
      "Iter:   77 | Cost: 0.0001197 | Accuracy: 0.2000000\n",
      "Iter:   78 | Cost: 0.0001199 | Accuracy: 0.1500000\n",
      "Iter:   79 | Cost: 0.0001166 | Accuracy: 0.1250000\n",
      "Iter:   80 | Cost: 0.0001107 | Accuracy: 0.1250000\n",
      "Iter:   81 | Cost: 0.0001034 | Accuracy: 0.1250000\n",
      "Iter:   82 | Cost: 0.0000943 | Accuracy: 0.1125000\n",
      "Iter:   83 | Cost: 0.0000824 | Accuracy: 0.1125000\n",
      "Iter:   84 | Cost: 0.0000696 | Accuracy: 0.1375000\n",
      "Iter:   85 | Cost: 0.0000560 | Accuracy: 0.1500000\n",
      "Iter:   86 | Cost: 0.0000440 | Accuracy: 0.1750000\n",
      "Iter:   87 | Cost: 0.0000335 | Accuracy: 0.2375000\n",
      "Iter:   88 | Cost: 0.0000246 | Accuracy: 0.3125000\n",
      "Iter:   89 | Cost: 0.0000173 | Accuracy: 0.3000000\n",
      "Iter:   90 | Cost: 0.0000118 | Accuracy: 0.3000000\n",
      "Iter:   91 | Cost: 0.0000075 | Accuracy: 0.2750000\n",
      "Iter:   92 | Cost: 0.0000044 | Accuracy: 0.1750000\n",
      "Iter:   93 | Cost: 0.0000025 | Accuracy: 0.0000000\n",
      "Iter:   94 | Cost: 0.0000014 | Accuracy: 0.1250000\n",
      "Iter:   95 | Cost: 0.0000010 | Accuracy: 0.5625000\n",
      "Iter:   96 | Cost: 0.0000010 | Accuracy: 0.6125000\n",
      "Iter:   97 | Cost: 0.0000014 | Accuracy: 0.4375000\n",
      "Iter:   98 | Cost: 0.0000018 | Accuracy: 0.3375000\n",
      "Iter:   99 | Cost: 0.0000023 | Accuracy: 0.2750000\n",
      "Iter:  100 | Cost: 0.0000027 | Accuracy: 0.2500000\n",
      "Iter:  101 | Cost: 0.0000030 | Accuracy: 0.2125000\n",
      "Iter:  102 | Cost: 0.0000032 | Accuracy: 0.2750000\n",
      "Iter:  103 | Cost: 0.0000034 | Accuracy: 0.2750000\n",
      "Iter:  104 | Cost: 0.0000035 | Accuracy: 0.3250000\n",
      "Iter:  105 | Cost: 0.0000034 | Accuracy: 0.3500000\n",
      "Iter:  106 | Cost: 0.0000033 | Accuracy: 0.4750000\n",
      "Iter:  107 | Cost: 0.0000031 | Accuracy: 0.4625000\n",
      "Iter:  108 | Cost: 0.0000029 | Accuracy: 0.4625000\n",
      "Iter:  109 | Cost: 0.0000027 | Accuracy: 0.4625000\n",
      "Iter:  110 | Cost: 0.0000024 | Accuracy: 0.4625000\n",
      "Iter:  111 | Cost: 0.0000021 | Accuracy: 0.4625000\n",
      "Iter:  112 | Cost: 0.0000019 | Accuracy: 0.4625000\n",
      "Iter:  113 | Cost: 0.0000016 | Accuracy: 0.4500000\n",
      "Iter:  114 | Cost: 0.0000014 | Accuracy: 0.4625000\n",
      "Iter:  115 | Cost: 0.0000012 | Accuracy: 0.4625000\n",
      "Iter:  116 | Cost: 0.0000011 | Accuracy: 0.4625000\n",
      "Iter:  117 | Cost: 0.0000009 | Accuracy: 0.6625000\n",
      "Iter:  118 | Cost: 0.0000008 | Accuracy: 1.0000000\n",
      "Iter:  119 | Cost: 0.0000007 | Accuracy: 1.0000000\n",
      "Iter:  120 | Cost: 0.0000006 | Accuracy: 0.8375000\n",
      "Iter:  121 | Cost: 0.0000006 | Accuracy: 0.8500000\n",
      "Iter:  122 | Cost: 0.0000005 | Accuracy: 0.8625000\n",
      "Iter:  123 | Cost: 0.0000004 | Accuracy: 0.9250000\n",
      "Iter:  124 | Cost: 0.0000004 | Accuracy: 1.0000000\n",
      "Iter:  125 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  126 | Cost: 0.0000003 | Accuracy: 1.0000000\n",
      "Iter:  127 | Cost: 0.0000002 | Accuracy: 1.0000000\n",
      "Iter:  128 | Cost: 0.0000002 | Accuracy: 1.0000000\n",
      "Iter:  129 | Cost: 0.0000002 | Accuracy: 1.0000000\n",
      "Iter:  130 | Cost: 0.0000001 | Accuracy: 1.0000000\n",
      "Iter:  131 | Cost: 0.0000001 | Accuracy: 1.0000000\n",
      "Iter:  132 | Cost: 0.0000001 | Accuracy: 1.0000000\n",
      "Iter:  133 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  134 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  135 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  136 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  137 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  138 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  139 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  140 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  141 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  142 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  143 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  144 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  145 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  146 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  147 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  148 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  149 | Cost: 0.0000000 | Accuracy: 1.0000000\n",
      "Iter:  150 | Cost: 0.0000000 | Accuracy: 1.0000000\n"
     ]
    }
   ],
   "source": [
    "opt = NesterovMomentumOptimizer(0.005)\n",
    "batch_size = 10\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(150):\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(features_train), (batch_size,))\n",
    "    features_batch = features_train[batch_index]\n",
    "    labels_batch = labels_train[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, features=features_batch, labels=labels_batch)\n",
    "    # Compute accuracy\n",
    "    predictions = [(variational_classifier(weights, bias, x)) for x in features_train]\n",
    "    current_cost = cost(weights, bias, features_train, labels_train)\n",
    "    acc = accuracy(labels_train, predictions, precision=0.001)\n",
    "\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 1.00\n"
     ]
    }
   ],
   "source": [
    "predictions = [(variational_classifier(weights, bias, x)) for x in features_test]\n",
    "acc = accuracy(labels_test, predictions)\n",
    "print(\"Accuracy on test data:\", f\"{acc:0.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved 100% accuracy with $0.001\\$ precision on on unseen data. We can use higher precision, but this requires carefully tuning the model parameters. Also, for more digits, using the model will be more expensive since then, we have to run the model many more times to get results within the desired precision.\n",
    "\n",
    "The main issue with this model is that it's not efficient to use. For extreme precision results, we have to run the circuit many times until our solution approaches a value withing our expected range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary fraction extraction $-$ multi qubit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The produced model from this approach encodes the solution in a binary string with `n` binary fractions. This means we measure more than one qubit. \n",
    "\n",
    "$sin(x) = \\alpha$ where in binary $\\alpha = a_0 . a_1a_2...a_n$\n",
    "\n",
    "$$\\alpha = a_0 2^0 + a_1 2^{-1} + a_2 2^{-2} + ... + a_n 2^{-n}$$\n",
    "$a_i\\in \\{0, 1\\}$\n",
    "\n",
    "Our goal is to design a model that accepts any given $n$.\n",
    "\n",
    "*Note: we might get away with one qubit and get the same results as if we have $n$ qubits if we find a way to recycle our qubit, similar to what is done in some phase estimation techniques.*\n",
    "\n",
    "\n",
    "To implement this approach, I can think of two main paths:\n",
    "1) Get the first n binary fractional digits with no approximation\n",
    "2) Get an approximation that has n fractional digits\n",
    "\n",
    "To see the importance of the difference, let's take $0.93$ as an example. In binary, the first four digits are: $$0.1110$$  which is $0.875$ in decimal. But the binary fraction $0.1111$ is $0.9375$ in decimal.\n",
    "\n",
    "I will be training my model to get the former approximation. In our `cost` function, I will use the difference between the generated values by our quantum circuit with the first `n` digits of the actual answer. This means that we don't really need to know the ideal binary approximation. \n",
    "\n",
    "*Note: my approach assumes that the accuracy of the decimal values are higher than the accuracy we intend to generate using `n` binary fractional digits.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# We use random data points with random spacing to make sure \n",
    "#   our model understands the behavior of sin everywhere\n",
    "num_p = 300 # number of data points\n",
    "features = np.random.uniform(0, 2 * np.pi, num_p)\n",
    "labels = np.sin(features)\n",
    "\n",
    "# Shuffle and split the data into training and test sets\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(num_p)\n",
    "split_index = int(num_p * 0.8) # 80% for training\n",
    "train_indices, test_indices = indices[:split_index], indices[split_index:]\n",
    "features_train = features[train_indices]\n",
    "features_test = features[test_indices]\n",
    "labels_train = labels[train_indices]\n",
    "labels_test = labels[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 # number of fractional binary digits\n",
    "num_qubits = (n + 2) # qubit for sign and qubit for first integer digit\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encode the data using a simple format:\n",
    "\n",
    "[sign, integer part of the binary number, `n` binary fractions to represent bits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the first n binary fractional digits with their sign \n",
    "# Output format [sign: {1 for + | -1 for -}, int digit: {1 for one | -1 for zero}, \n",
    "#                       fractional n digits: {1 for one | -1 for zero} ...]\n",
    "float_to_binary = lambda num, precision: (\n",
    "    [1 if num >= 0 else -1] + # sign\n",
    "    [1 if num == 1. else -1] +  # 1 if bit = 1, -1 if bit = 0\n",
    "        [1 if (num := (abs(num) % 1) * 2) >= 1 else -1 for _ in range(precision)] \n",
    "        )\n",
    "# Inverses the previous function to get decimal values\n",
    "binary_to_float = lambda binary_rep: (1 if binary_rep[0] == 1 else -1) * sum((bit == 1) * 2**(-i) for i, bit in enumerate(binary_rep[2:], start=1))\n",
    "\n",
    "\n",
    "features_train2 = np.array([float_to_binary(feature, n) for feature in features_train])\n",
    "features_test2 = np.array([float_to_binary(feature, n) for feature in features_test])\n",
    "# Labels are generated using the binary representation of the input\n",
    "labels_train2 = np.array( [float_to_binary(feature, n)for feature in  \n",
    "                           [np.sin(binary_to_float(i)) for i in features_train2]\n",
    "                           ] \n",
    "                         )\n",
    "labels_test2 = np.array( [float_to_binary(feature, n)for feature in  \n",
    "                           [np.sin(binary_to_float(i)) for i in features_test2]\n",
    "                           ] \n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the only feature we are using is the input $x$, the angle for which we want to find $sin(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_preparation(features2):\n",
    "    for i, state in enumerate(features2):\n",
    "        if state == 1.:\n",
    "            qml.X(i)\n",
    "\n",
    "def layer(layer_weights, num_qubits):\n",
    "    for i in range(0, num_qubits):\n",
    "        qml.Rot(*layer_weights[i], wires=i)\n",
    "    for i in range(0, num_qubits - 1):\n",
    "        qml.CNOT([i, (i + 1)])\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(weights, feature, num_qubits):\n",
    "    state_preparation(feature)\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights, num_qubits)\n",
    "    return np.array([qml.expval(qml.PauliZ(i)) for i in range(num_qubits)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a model to get the answer we want. `cost` needs some modifications. We change `square_loss` to find the difference between `label` and the solution produced by the model. `cost` gives weight to each input based on their position in the list, with the first entries having higher cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions_bits):\n",
    "    # We give more weight to the first entries\n",
    "    return sum([ \n",
    "        sum(\n",
    "            [ (l - p) ** 2 / (i + 1)   for i, (l,p) in enumerate(zip(label, prediction))]\n",
    "            )  \n",
    "                for label, prediction in zip(labels, predictions_bits)\n",
    "                ]) / len(labels)\n",
    "    \n",
    "def variational_classifier(weights, bias, feature, num_qubits):\n",
    "    return circuit(weights, feature, num_qubits) + bias\n",
    "\n",
    "def cost(weights, bias, features, labels, num_qubits):\n",
    "    return square_loss(labels, \n",
    "                       np.array([variational_classifier(weights, bias, feature, num_qubits) \n",
    "                                 for feature in features], requires_grad=True))\n",
    "def accuracy(predictions, labels):\n",
    "    l = sum(1 if np.all(np.sign(np.array(p)) == np.array(l)) else 0 for (p, l) in zip(predictions, labels))   \n",
    "    return l / len(predictions) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something interesting happens in `square_loss`. `bits` are expectation values $\\in[-1,1]$. We don't round them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:    1 | Cost: 8.2067449 | Acc: 0.00000 %\n",
      "Iter:    2 | Cost: 8.1761092 | Acc: 0.00000 %\n",
      "Iter:    3 | Cost: 8.1248863 | Acc: 0.00000 %\n",
      "Iter:    4 | Cost: 7.9126442 | Acc: 0.00000 %\n",
      "Iter:    5 | Cost: 6.9182861 | Acc: 0.00000 %\n",
      "Iter:    6 | Cost: 4.0544131 | Acc: 0.00000 %\n",
      "Iter:    7 | Cost: 1.6396671 | Acc: 0.00000 %\n",
      "Iter:    8 | Cost: 1.0811020 | Acc: 47.50000 %\n",
      "Iter:    9 | Cost: 1.1297739 | Acc: 47.50000 %\n",
      "Iter:   10 | Cost: 1.1665300 | Acc: 47.50000 %\n",
      "Iter:   11 | Cost: 1.1119887 | Acc: 26.25000 %\n",
      "Iter:   12 | Cost: 0.9949251 | Acc: 39.58333 %\n",
      "Iter:   13 | Cost: 0.8837914 | Acc: 53.33333 %\n",
      "Iter:   14 | Cost: 0.8346043 | Acc: 53.33333 %\n",
      "Iter:   15 | Cost: 0.8261692 | Acc: 40.41667 %\n",
      "Iter:   16 | Cost: 0.8326259 | Acc: 40.41667 %\n",
      "Iter:   17 | Cost: 0.8185416 | Acc: 40.41667 %\n",
      "Iter:   18 | Cost: 0.7875008 | Acc: 40.41667 %\n",
      "Iter:   19 | Cost: 0.7445721 | Acc: 40.41667 %\n",
      "Iter:   20 | Cost: 0.7088951 | Acc: 40.41667 %\n",
      "Iter:   21 | Cost: 0.6781153 | Acc: 36.66667 %\n",
      "Iter:   22 | Cost: 0.6586208 | Acc: 36.66667 %\n",
      "Iter:   23 | Cost: 0.6390464 | Acc: 47.50000 %\n",
      "Iter:   24 | Cost: 0.6190015 | Acc: 47.50000 %\n",
      "Iter:   25 | Cost: 0.5995474 | Acc: 47.50000 %\n",
      "Iter:   26 | Cost: 0.5796700 | Acc: 47.50000 %\n",
      "Iter:   27 | Cost: 0.5603339 | Acc: 47.50000 %\n",
      "Iter:   28 | Cost: 0.5456639 | Acc: 47.50000 %\n",
      "Iter:   29 | Cost: 0.5365281 | Acc: 47.50000 %\n",
      "Iter:   30 | Cost: 0.5325512 | Acc: 47.50000 %\n",
      "Iter:   31 | Cost: 0.5306942 | Acc: 47.50000 %\n",
      "Iter:   32 | Cost: 0.5269757 | Acc: 47.50000 %\n",
      "Iter:   33 | Cost: 0.5216401 | Acc: 47.50000 %\n",
      "Iter:   34 | Cost: 0.5121510 | Acc: 47.50000 %\n",
      "Iter:   35 | Cost: 0.5037059 | Acc: 47.50000 %\n",
      "Iter:   36 | Cost: 0.4946553 | Acc: 47.50000 %\n",
      "Iter:   37 | Cost: 0.4847819 | Acc: 47.50000 %\n",
      "Iter:   38 | Cost: 0.4773857 | Acc: 47.50000 %\n",
      "Iter:   39 | Cost: 0.4709830 | Acc: 47.50000 %\n",
      "Iter:   40 | Cost: 0.4652146 | Acc: 47.50000 %\n",
      "Iter:   41 | Cost: 0.4627044 | Acc: 47.50000 %\n",
      "Iter:   42 | Cost: 0.4598358 | Acc: 47.50000 %\n",
      "Iter:   43 | Cost: 0.4540019 | Acc: 47.50000 %\n",
      "Iter:   44 | Cost: 0.4438458 | Acc: 47.50000 %\n",
      "Iter:   45 | Cost: 0.4320309 | Acc: 47.50000 %\n",
      "Iter:   46 | Cost: 0.4191860 | Acc: 59.58333 %\n",
      "Iter:   47 | Cost: 0.4056631 | Acc: 59.58333 %\n",
      "Iter:   48 | Cost: 0.3930201 | Acc: 59.58333 %\n",
      "Iter:   49 | Cost: 0.3826591 | Acc: 59.58333 %\n",
      "Iter:   50 | Cost: 0.3736476 | Acc: 59.58333 %\n",
      "Iter:   51 | Cost: 0.3642348 | Acc: 59.58333 %\n",
      "Iter:   52 | Cost: 0.3547145 | Acc: 59.58333 %\n",
      "Iter:   53 | Cost: 0.3424711 | Acc: 59.58333 %\n",
      "Iter:   54 | Cost: 0.3298099 | Acc: 59.58333 %\n",
      "Iter:   55 | Cost: 0.3092834 | Acc: 73.33333 %\n",
      "Iter:   56 | Cost: 0.2883793 | Acc: 86.66667 %\n",
      "Iter:   57 | Cost: 0.2747030 | Acc: 86.66667 %\n",
      "Iter:   58 | Cost: 0.2693274 | Acc: 86.66667 %\n",
      "Iter:   59 | Cost: 0.2619785 | Acc: 74.58333 %\n",
      "Iter:   60 | Cost: 0.2554894 | Acc: 74.58333 %\n",
      "Iter:   61 | Cost: 0.2498194 | Acc: 74.58333 %\n",
      "Iter:   62 | Cost: 0.2414001 | Acc: 86.66667 %\n",
      "Iter:   63 | Cost: 0.2333085 | Acc: 86.66667 %\n",
      "Iter:   64 | Cost: 0.2284583 | Acc: 74.58333 %\n",
      "Iter:   65 | Cost: 0.2286838 | Acc: 74.58333 %\n",
      "Iter:   66 | Cost: 0.2207330 | Acc: 74.58333 %\n",
      "Iter:   67 | Cost: 0.2140052 | Acc: 74.58333 %\n",
      "Iter:   68 | Cost: 0.2094187 | Acc: 74.58333 %\n",
      "Iter:   69 | Cost: 0.2059901 | Acc: 74.58333 %\n",
      "Iter:   70 | Cost: 0.2041522 | Acc: 74.58333 %\n",
      "Iter:   71 | Cost: 0.2027862 | Acc: 74.58333 %\n",
      "Iter:   72 | Cost: 0.2021890 | Acc: 74.58333 %\n",
      "Iter:   73 | Cost: 0.2010894 | Acc: 74.58333 %\n",
      "Iter:   74 | Cost: 0.1988955 | Acc: 74.58333 %\n",
      "Iter:   75 | Cost: 0.1962025 | Acc: 86.66667 %\n",
      "Iter:   76 | Cost: 0.1966031 | Acc: 86.66667 %\n",
      "Iter:   77 | Cost: 0.1933131 | Acc: 86.66667 %\n",
      "Iter:   78 | Cost: 0.1943291 | Acc: 86.66667 %\n",
      "Iter:   79 | Cost: 0.1958485 | Acc: 86.66667 %\n",
      "Iter:   80 | Cost: 0.2001124 | Acc: 100.00000 %\n",
      "Iter:   81 | Cost: 0.2016304 | Acc: 100.00000 %\n",
      "Iter:   82 | Cost: 0.2007881 | Acc: 100.00000 %\n",
      "Iter:   83 | Cost: 0.1979568 | Acc: 100.00000 %\n",
      "Iter:   84 | Cost: 0.1952527 | Acc: 86.66667 %\n",
      "Iter:   85 | Cost: 0.1939478 | Acc: 86.66667 %\n",
      "Iter:   86 | Cost: 0.1930806 | Acc: 86.66667 %\n",
      "Iter:   87 | Cost: 0.1935921 | Acc: 86.66667 %\n",
      "Iter:   88 | Cost: 0.1953330 | Acc: 86.66667 %\n",
      "Iter:   89 | Cost: 0.1983861 | Acc: 86.66667 %\n",
      "Iter:   90 | Cost: 0.2023144 | Acc: 86.66667 %\n",
      "Iter:   91 | Cost: 0.2070286 | Acc: 86.66667 %\n",
      "Iter:   92 | Cost: 0.2065911 | Acc: 86.66667 %\n",
      "Iter:   93 | Cost: 0.2045625 | Acc: 86.66667 %\n",
      "Iter:   94 | Cost: 0.2016005 | Acc: 86.66667 %\n",
      "Iter:   95 | Cost: 0.2017904 | Acc: 86.66667 %\n",
      "Iter:   96 | Cost: 0.2005570 | Acc: 86.66667 %\n",
      "Iter:   97 | Cost: 0.1972370 | Acc: 86.66667 %\n",
      "Iter:   98 | Cost: 0.1949156 | Acc: 86.66667 %\n",
      "Iter:   99 | Cost: 0.1926730 | Acc: 86.66667 %\n",
      "Iter:  100 | Cost: 0.1918431 | Acc: 86.66667 %\n",
      "Iter:  101 | Cost: 0.1920238 | Acc: 86.66667 %\n",
      "Iter:  102 | Cost: 0.1922611 | Acc: 86.66667 %\n",
      "Iter:  103 | Cost: 0.1927964 | Acc: 86.66667 %\n",
      "Iter:  104 | Cost: 0.1921802 | Acc: 86.66667 %\n",
      "Iter:  105 | Cost: 0.1906024 | Acc: 86.66667 %\n",
      "Iter:  106 | Cost: 0.1899278 | Acc: 86.66667 %\n",
      "Iter:  107 | Cost: 0.1894655 | Acc: 86.66667 %\n",
      "Iter:  108 | Cost: 0.1896326 | Acc: 86.66667 %\n",
      "Iter:  109 | Cost: 0.1890256 | Acc: 86.66667 %\n",
      "Iter:  110 | Cost: 0.1889090 | Acc: 86.66667 %\n",
      "Iter:  111 | Cost: 0.1887772 | Acc: 86.66667 %\n",
      "Iter:  112 | Cost: 0.1887639 | Acc: 86.66667 %\n",
      "Iter:  113 | Cost: 0.1888689 | Acc: 86.66667 %\n",
      "Iter:  114 | Cost: 0.1891505 | Acc: 86.66667 %\n",
      "Iter:  115 | Cost: 0.1897157 | Acc: 86.66667 %\n",
      "Iter:  116 | Cost: 0.1906888 | Acc: 86.66667 %\n",
      "Iter:  117 | Cost: 0.1907293 | Acc: 86.66667 %\n",
      "Iter:  118 | Cost: 0.1912577 | Acc: 86.66667 %\n",
      "Iter:  119 | Cost: 0.1903263 | Acc: 86.66667 %\n",
      "Iter:  120 | Cost: 0.1904651 | Acc: 86.66667 %\n",
      "Iter:  121 | Cost: 0.1912027 | Acc: 86.66667 %\n",
      "Iter:  122 | Cost: 0.1916108 | Acc: 86.66667 %\n",
      "Iter:  123 | Cost: 0.1910721 | Acc: 86.66667 %\n",
      "Iter:  124 | Cost: 0.1913381 | Acc: 86.66667 %\n",
      "Iter:  125 | Cost: 0.1910166 | Acc: 86.66667 %\n",
      "Iter:  126 | Cost: 0.1909741 | Acc: 86.66667 %\n",
      "Iter:  127 | Cost: 0.1889651 | Acc: 86.66667 %\n",
      "Iter:  128 | Cost: 0.1890247 | Acc: 86.66667 %\n",
      "Iter:  129 | Cost: 0.1893127 | Acc: 86.66667 %\n",
      "Iter:  130 | Cost: 0.1904990 | Acc: 86.66667 %\n",
      "Iter:  131 | Cost: 0.1919169 | Acc: 100.00000 %\n",
      "Iter:  132 | Cost: 0.1931634 | Acc: 100.00000 %\n",
      "Iter:  133 | Cost: 0.1950116 | Acc: 100.00000 %\n",
      "Iter:  134 | Cost: 0.1992651 | Acc: 100.00000 %\n",
      "Iter:  135 | Cost: 0.2028656 | Acc: 100.00000 %\n",
      "Iter:  136 | Cost: 0.2055400 | Acc: 100.00000 %\n",
      "Iter:  137 | Cost: 0.2029125 | Acc: 100.00000 %\n",
      "Iter:  138 | Cost: 0.1986987 | Acc: 100.00000 %\n",
      "Iter:  139 | Cost: 0.1927794 | Acc: 100.00000 %\n",
      "Iter:  140 | Cost: 0.1900872 | Acc: 86.66667 %\n",
      "Iter:  141 | Cost: 0.1889361 | Acc: 86.66667 %\n",
      "Iter:  142 | Cost: 0.1910625 | Acc: 86.66667 %\n",
      "Iter:  143 | Cost: 0.1902101 | Acc: 86.66667 %\n",
      "Iter:  144 | Cost: 0.1895183 | Acc: 86.66667 %\n",
      "Iter:  145 | Cost: 0.1892258 | Acc: 86.66667 %\n",
      "Iter:  146 | Cost: 0.1896163 | Acc: 86.66667 %\n",
      "Iter:  147 | Cost: 0.1892598 | Acc: 86.66667 %\n",
      "Iter:  148 | Cost: 0.1900437 | Acc: 86.66667 %\n",
      "Iter:  149 | Cost: 0.1914027 | Acc: 86.66667 %\n",
      "Iter:  150 | Cost: 0.1924058 | Acc: 86.66667 %\n",
      "Iter:  151 | Cost: 0.1911286 | Acc: 86.66667 %\n",
      "Iter:  152 | Cost: 0.1906401 | Acc: 86.66667 %\n",
      "Iter:  153 | Cost: 0.1893030 | Acc: 86.66667 %\n",
      "Iter:  154 | Cost: 0.1879237 | Acc: 86.66667 %\n",
      "Iter:  155 | Cost: 0.1903218 | Acc: 86.66667 %\n",
      "Iter:  156 | Cost: 0.1969585 | Acc: 100.00000 %\n",
      "Iter:  157 | Cost: 0.2035772 | Acc: 100.00000 %\n",
      "Iter:  158 | Cost: 0.2054131 | Acc: 100.00000 %\n",
      "Iter:  159 | Cost: 0.2061282 | Acc: 100.00000 %\n",
      "Iter:  160 | Cost: 0.2009198 | Acc: 100.00000 %\n",
      "Iter:  161 | Cost: 0.1964604 | Acc: 100.00000 %\n",
      "Iter:  162 | Cost: 0.1937178 | Acc: 100.00000 %\n",
      "Iter:  163 | Cost: 0.1924701 | Acc: 100.00000 %\n",
      "Iter:  164 | Cost: 0.1927761 | Acc: 74.58333 %\n",
      "Iter:  165 | Cost: 0.1915875 | Acc: 86.66667 %\n",
      "Iter:  166 | Cost: 0.1918939 | Acc: 86.66667 %\n",
      "Iter:  167 | Cost: 0.1930983 | Acc: 86.66667 %\n",
      "Iter:  168 | Cost: 0.1952386 | Acc: 74.58333 %\n",
      "Iter:  169 | Cost: 0.1967869 | Acc: 74.58333 %\n",
      "Iter:  170 | Cost: 0.1972111 | Acc: 74.58333 %\n",
      "Iter:  171 | Cost: 0.1978853 | Acc: 74.58333 %\n",
      "Iter:  172 | Cost: 0.1946041 | Acc: 74.58333 %\n",
      "Iter:  173 | Cost: 0.1929922 | Acc: 86.66667 %\n",
      "Iter:  174 | Cost: 0.1907861 | Acc: 86.66667 %\n",
      "Iter:  175 | Cost: 0.1897116 | Acc: 86.66667 %\n",
      "Iter:  176 | Cost: 0.1884375 | Acc: 86.66667 %\n",
      "Iter:  177 | Cost: 0.1883052 | Acc: 86.66667 %\n",
      "Iter:  178 | Cost: 0.1890515 | Acc: 86.66667 %\n",
      "Iter:  179 | Cost: 0.1910433 | Acc: 86.66667 %\n",
      "Iter:  180 | Cost: 0.1937863 | Acc: 86.66667 %\n",
      "Iter:  181 | Cost: 0.1944641 | Acc: 86.66667 %\n",
      "Iter:  182 | Cost: 0.1928853 | Acc: 86.66667 %\n",
      "Iter:  183 | Cost: 0.1931020 | Acc: 86.66667 %\n",
      "Iter:  184 | Cost: 0.1919409 | Acc: 86.66667 %\n",
      "Iter:  185 | Cost: 0.1923256 | Acc: 86.66667 %\n",
      "Iter:  186 | Cost: 0.1924984 | Acc: 86.66667 %\n",
      "Iter:  187 | Cost: 0.1901185 | Acc: 86.66667 %\n",
      "Iter:  188 | Cost: 0.1891513 | Acc: 86.66667 %\n",
      "Iter:  189 | Cost: 0.1886207 | Acc: 86.66667 %\n",
      "Iter:  190 | Cost: 0.1877539 | Acc: 86.66667 %\n",
      "Iter:  191 | Cost: 0.1878297 | Acc: 86.66667 %\n",
      "Iter:  192 | Cost: 0.1891749 | Acc: 100.00000 %\n",
      "Iter:  193 | Cost: 0.1921163 | Acc: 100.00000 %\n",
      "Iter:  194 | Cost: 0.1939968 | Acc: 100.00000 %\n",
      "Iter:  195 | Cost: 0.1939009 | Acc: 100.00000 %\n",
      "Iter:  196 | Cost: 0.1923139 | Acc: 100.00000 %\n",
      "Iter:  197 | Cost: 0.1906242 | Acc: 100.00000 %\n",
      "Iter:  198 | Cost: 0.1908770 | Acc: 100.00000 %\n",
      "Iter:  199 | Cost: 0.1916486 | Acc: 100.00000 %\n",
      "Iter:  200 | Cost: 0.1912753 | Acc: 87.91667 %\n",
      "Iter:  201 | Cost: 0.1893337 | Acc: 100.00000 %\n",
      "Iter:  202 | Cost: 0.1888095 | Acc: 87.91667 %\n",
      "Iter:  203 | Cost: 0.1880690 | Acc: 100.00000 %\n",
      "Iter:  204 | Cost: 0.1877496 | Acc: 100.00000 %\n",
      "Iter:  205 | Cost: 0.1870956 | Acc: 87.91667 %\n",
      "Iter:  206 | Cost: 0.1868362 | Acc: 74.58333 %\n",
      "Iter:  207 | Cost: 0.1899366 | Acc: 74.58333 %\n",
      "Iter:  208 | Cost: 0.1896019 | Acc: 74.58333 %\n",
      "Iter:  209 | Cost: 0.1866891 | Acc: 74.58333 %\n",
      "Iter:  210 | Cost: 0.1865000 | Acc: 74.58333 %\n",
      "Iter:  211 | Cost: 0.1876324 | Acc: 86.66667 %\n",
      "Iter:  212 | Cost: 0.1866412 | Acc: 86.66667 %\n",
      "Iter:  213 | Cost: 0.1846701 | Acc: 86.66667 %\n",
      "Iter:  214 | Cost: 0.1843937 | Acc: 86.66667 %\n",
      "Iter:  215 | Cost: 0.1860837 | Acc: 86.66667 %\n",
      "Iter:  216 | Cost: 0.1870116 | Acc: 100.00000 %\n",
      "Iter:  217 | Cost: 0.1883235 | Acc: 100.00000 %\n",
      "Iter:  218 | Cost: 0.1904027 | Acc: 100.00000 %\n",
      "Iter:  219 | Cost: 0.1933424 | Acc: 100.00000 %\n",
      "Iter:  220 | Cost: 0.1898791 | Acc: 100.00000 %\n",
      "Iter:  221 | Cost: 0.1892988 | Acc: 100.00000 %\n",
      "Iter:  222 | Cost: 0.1875780 | Acc: 100.00000 %\n",
      "Iter:  223 | Cost: 0.1862807 | Acc: 100.00000 %\n",
      "Iter:  224 | Cost: 0.1861772 | Acc: 100.00000 %\n",
      "Iter:  225 | Cost: 0.1877313 | Acc: 100.00000 %\n",
      "Iter:  226 | Cost: 0.1883529 | Acc: 100.00000 %\n",
      "Iter:  227 | Cost: 0.1871258 | Acc: 100.00000 %\n",
      "Iter:  228 | Cost: 0.1856846 | Acc: 100.00000 %\n",
      "Iter:  229 | Cost: 0.1844285 | Acc: 100.00000 %\n",
      "Iter:  230 | Cost: 0.1847237 | Acc: 100.00000 %\n",
      "Iter:  231 | Cost: 0.1856525 | Acc: 100.00000 %\n",
      "Iter:  232 | Cost: 0.1853190 | Acc: 100.00000 %\n",
      "Iter:  233 | Cost: 0.1839687 | Acc: 100.00000 %\n",
      "Iter:  234 | Cost: 0.1868639 | Acc: 100.00000 %\n",
      "Iter:  235 | Cost: 0.1888644 | Acc: 100.00000 %\n",
      "Iter:  236 | Cost: 0.1883553 | Acc: 100.00000 %\n",
      "Iter:  237 | Cost: 0.1868585 | Acc: 100.00000 %\n",
      "Iter:  238 | Cost: 0.1827971 | Acc: 100.00000 %\n",
      "Iter:  239 | Cost: 0.1818933 | Acc: 100.00000 %\n",
      "Iter:  240 | Cost: 0.1849841 | Acc: 100.00000 %\n",
      "Iter:  241 | Cost: 0.1845748 | Acc: 100.00000 %\n",
      "Iter:  242 | Cost: 0.1832938 | Acc: 100.00000 %\n",
      "Iter:  243 | Cost: 0.1827560 | Acc: 100.00000 %\n",
      "Iter:  244 | Cost: 0.1833461 | Acc: 100.00000 %\n",
      "Iter:  245 | Cost: 0.1834961 | Acc: 100.00000 %\n",
      "Iter:  246 | Cost: 0.1837278 | Acc: 100.00000 %\n",
      "Iter:  247 | Cost: 0.1822108 | Acc: 100.00000 %\n",
      "Iter:  248 | Cost: 0.1821271 | Acc: 100.00000 %\n",
      "Iter:  249 | Cost: 0.1825012 | Acc: 100.00000 %\n",
      "Iter:  250 | Cost: 0.1826912 | Acc: 100.00000 %\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.5, requires_grad=True)\n",
    "opt = NesterovMomentumOptimizer(0.1)\n",
    "batch_size = 10\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(250):\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(features_train2), (batch_size,))\n",
    "    features_batch = features_train2[batch_index]\n",
    "    labels_batch = labels_train2[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, features=features_batch, labels=labels_batch, num_qubits=num_qubits)\n",
    "    current_cost = cost(weights, bias, features_train2, labels_train2, num_qubits)\n",
    "    # Compute accuracy\n",
    "    predictions = [(variational_classifier(weights, bias, x, num_qubits)) for x in features_train2]\n",
    "    acc = accuracy(predictions, labels_train2)\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Acc: {acc:0.5f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clarify the meaning of the output here, it is not about expectation values like in the previous model, but about what is more likely, a zero or a one. We run the model multiple times, if a qubit has higher probability to be one, then this qubit's output is one. The benefit of this approach is that we can achieve perfect accuracy without achieving 0 cost. Though, 0 cost is of course better and means one shot is enough to get the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "predictions = [(variational_classifier(weights, bias, x,num_qubits)) for x in features_test2]\n",
    "predictions = [np.array([int(np.sign(bit)) for bit in prediction]) for prediction in predictions]\n",
    "\n",
    "print(\"Accuracy on test data:\", accuracy(predictions, labels_test2) , \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to approximate any function\n",
    "If interested, you should check this Pennylane tutorial to approximate sin using fourier analysis\n",
    "https://pennylane.ai/qml/demos/tutorial_expressivity_fourier_series/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
